\section{Scikit-Learn}

Jupyter notebook examples can be found here: \textit{https://github.com/ageron/handson-ml2}\newline

%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Introduction}

\underline{What is Machine Learning?}
\begin{itemize}
\vspace{-4.0mm}
\item
ML is about making machines get better at some task by learning from data,
instead of having to explicitly code rules.
\item
A computer program is said to learn from experience E with respect to some task T and some performance measure P,
if its performance on T, as measured by P, improves with experience E.
\end{itemize}

\vspace{-4.0mm}
\underline{Jargon:}
\begin{itemize}
\vspace{-4.0mm}
\item
\textbf{Regression:} predicting values using features/predictors.
\item
\textbf{Classification:} predicting classifications using features/predictors.
\item
\textbf{Supervised learning:}
the training data includes the desired solutions, called labels.\newline
\textit{The label is either a target value or a classification.}
\item
\textbf{Unsupervised learning:}
the training data is unlabeled.\newline
\textit{E.g. clustering, anomaly detection, visualization, and dimensionality reduction.}
\item
\textbf{Semi-supervised learning:}
the training data is partially labeled.\newline
\textit{This is normally performed by a combination of unsupervised and supervised algorithms.}
\item
\textbf{Reinforcement learning:}
the agent observes the environment, selects/performs actions, and gets rewards/penalties in return.
Over time it learns the best strategy to maximize reward.
\item
\textbf{Batch/Offline learning:}
process all the data in one go.
\item
\textbf{Online learning:}
the system is trained incrementally by feeding it data sequentially.
\item
\textbf{Data mining:}
applying ML techniques to dig into large amounts of data
to help discover patterns that were not immediately apparent.
\item
\textbf{Cost function:}
measures how bad your model is.
\item
\textbf{Hyperparameters:}
the parameters of the learning algorithm (not the resultant model).\newline
\textit{They must be set prior to training.}
\item
\textbf{Inference:}
appyling a model to make predictions on new cases.
\item
\textbf{Feature engineering:}
is the combination of...\newline
- Feature selection: selecting the most useful features among existing features.\newline
- Feature extraction: combining existing features to produce a more useful one.\newline
- Creating new features by gathering new data.
\end{itemize}

\newpage
\underline{The main challenges of ML:}
\begin{enumerate}
\vspace{-4.0mm}
\item
\textbf{Bad data:}
\begin{itemize}
\vspace{-2.0mm}
\item
Insufficient quantity of data.
\item
Non-representative training data (sampling bias and/or sampling noise).
\item
Poor quality data (errors, outliers, and noise).
\item
Irrelevant features (garbage in = garbage out).
\end{itemize}
\item
\textbf{Bad algorithm:}
\begin{itemize}
\vspace{-2.0mm}
\item
\underline{Overfitting} the training data.\newline
\textit{The model performs well on training data, but it does not generalise well.}\newline
This happens when the model is too complex relative to the amount and the noisiness of the training data.
Solutions:\newline
- Select a simpler model or constrain your current model through regularization.\newline
- Obtain more training data and/or reduce the noise in the training data.
\item
\underline{Underfitting} the training data.\newline
\textit{The model is too simple to learn the underlying structure of the data.}
Solutions:\newline
- Reduce constraints on your model or select a more powerful model.\newline
- Feed better features to the learning algorithm (feature engineering).
\end{itemize}
\end{enumerate}

\vspace{+3.0mm}
\underline{Testing and validating a ML model}
\begin{itemize}
\vspace{-3.0mm}
\item
The only way to know how well a model generalizes, is to actually try it out on new cases.
\item
Split the data into two sets: a training set and a test set (typically with a 80:20 split).\newline
\textit{The error on test set is called the generalization error.
If the training error is low, but the generalization error is high, it means you are overfitting the training data.}
\end{itemize}

\vspace{+3.0mm}
\underline{Hyperparameter tuning and model selection}
\begin{itemize}
\vspace{-3.0mm}
\item
You cannot use the test set to tune hyperparameters or select a model.\newline
\textit{In doing so, your model would just adapt to that particular set,
and your generalization error will no longer be a valid estimate.}
\item
Instead, use a validation (development) set from part of the training set.
\item
\textbf{Cross validation} uses many small validation sets.
Each model is evaluated once per validation set after it is trained on the rest of the data.
By averaging out all the evaluations of a model,
you get a much more accurate measure of its performance.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{End-to-end ML (Regression)}

To avoid a data snooping bias, immediately create a randomized test set.\newline
\texttt{from sklearn.model\char`_selection import train\char`_test\char`_split}\newline
\texttt{train, test = train\char`_test\char`_split(data, test\char`_size=0.2, random\char`_state=42)}\newline

Exploratory analysis to gain insights.
\begin{itemize}
\vspace{-3.0mm}
\item
Plot distributions:\newline
\texttt{train.hist(bins=50, figsize=(20,15))} 
\item
Plot correlations:\newline
\texttt{from pandas.plotting import scatter\char`_matrix}\newline
\texttt{scatter\char`_matrix(train, figsize=(12,8))}
\item
Study correlation coefficients:\newline
\texttt{corr\char`_matrix = train.corr()}\newline
\texttt{corr\char`_matrix[\textquotesingle median\char`_house\char`_value\textquotesingle].sort\char`_values(ascending=False)}
\item
Transform attributes and/or try out various combinations of attributes:\newline
\texttt{train[\textquotesingle rooms\char`_per\char`_house\textquotesingle] = train[\textquotesingle total\char`_rooms\textquotesingle]/train[\textquotesingle houses\textquotesingle]}\newline
\texttt{train[\textquotesingle mean\char`_salary\char`_log\textquotesingle] = np.log(train[\textquotesingle mean\char`_salary\textquotesingle]+1.0)}\newline
\end{itemize}

Prepare the data for ML algorithms (wrap up as a function you can also apply to the test set).
\vspace{-3.0mm}
\begin{itemize}
\item
Data cleaning (address any NULL values):\newline
\texttt{train.dropna(subset=[\textquotesingle total\char`_rooms\textquotesingle])}~~~~get rid of the corresponding rows.\newline
\texttt{train.drop(\textquotesingle total\char`_rooms\textquotesingle, axis=1)}~~~~~~~~~~get rid of the entire attribute.\newline
\texttt{median = train[\textquotesingle total\char`_rooms\textquotesingle].median()}~~~~~~~~set to some value.\newline
\texttt{train[\textquotesingle total\char`_rooms\textquotesingle].fillna(median, inplace=True)}
\item
Separate the features and the labels:\newline
\texttt{train\char`_X = train.drop(\textquotesingle median\char`_house\char`_value\textquotesingle, axis=1)}\newline
\texttt{train\char`_y = train[\textquotesingle median\char`_house\char`_value\textquotesingle].copy()}
\item
Handle text/categorical attributes:\newline
Sometimes you can map categories onto numbers (e.g. bad, average, good, excellent).\newline
But you might have to One Hot Encode;
where you have one binary attribute per category.\newline
\textit{Note: one-hot-encoding can result in lots of input features
which slows down the model and degrades performance.}
\item
Feature Scaling:\newline
ML algos don't perform well when the input numerical features have very different scales.
\textbf{Min-max scaling (normalization)} $\rightarrow$ rescales between 0 and 1.\newline
\textit{This can be affected by outliers (consequently, you might want to take a features log first).}\newline
\textbf{Standardization} $\rightarrow$ subtracts the mean, and divides by the standard deviation.\newline
\textit{This gives a distribution with mean=0 and unit variance.
It does not bound values to a specific range (a problem with neural networks)
but is much less affected by outliers.}\newline

\texttt{from sklearn.preprocessing import MinMaxScaler \# not used here}\newline
\texttt{from sklearn.preprocessing import StandardScaler}\newline
\texttt{from sklearn.preprocessing import OneHotEncoder}\newline
\texttt{from sklearn.compose import ColumnTransformer}\newline
\newline
\texttt{cat\char`_attribs = [\textquotesingle ocean\char`_proximity\textquotesingle]}\newline
\texttt{num\char`_attribs = list(train\char`_X.drop(cat\char`_attribs, axis=1))}\newline
\newline
\texttt{full\char`_pipeline = ColumnTransformer([}\newline
\texttt{(\textquotesingle num\textquotesingle, StandardScaler(), num\char`_attribs),}\newline
\texttt{(\textquotesingle cat\textquotesingle, OneHotEncoder(), cat\char`_attribs),}\newline
\texttt{])}\newline
\newline
\texttt{train\char`_X\char`_prepared = full\char`_pipeline.fit\char`_transform(train\char`_X)}\newline
\item
To determine the order of the one-hot-encoded attributes, do the following...\newline
\texttt{cat\char`_encoder = full\char`_pipeline.named\char`_transformers\char`_[\textquotesingle cat\textquotesingle]}\newline
\texttt{cat\char`_encoder.categories\char`_[i]}
\end{itemize}

Training a model.
\vspace{-3.0mm}
\begin{itemize}
\item
Linear regression model:\newline
\texttt{from sklearn.linear\char`_model import LinearRegression}\newline
\texttt{model = LinearRegression()}\newline
\texttt{model.fit(train\char`_X\char`_prepared, train\char`_y)}
\item
Decision tree model:\newline
\texttt{from sklearn.tree import DecisionTreeRegressor}\newline
\texttt{model = DecisionTreeRegressor()}\newline
\texttt{model.fit(train\char`_X\char`_prepared, train\char`_y)}
% \item
% Random forest model:\newline
% \texttt{from sklearn.ensemble import RandomForestRegressor}\newline
% \texttt{model = RandomForestRegressor()}\newline
% \texttt{model.fit(train\char`_X\char`_prepared, train\char`_y)}
\end{itemize}

Selecting a model.
\vspace{-3.0mm}
\begin{itemize}
\item
Evaluate a model using the Root Mean Square Error (RMSE):\newline
\texttt{from sklearn.metrics import mean\char`_squared\char`_error}\newline
\texttt{train\char`_pred\char`_y = model.predict(train\char`_X\char`_prepared)}\newline
\texttt{mse = mean\char`_square\char`_error(train\char`_y, train\char`_pred\char`_y)}\newline
\texttt{rmse = np.sqrt(mse)}
\item
K-fold cross-validation (note that this doesn't result in the model being trained):\newline
- Splits the training set into $N$ distinct subsets called folds (represented by \texttt{cv} below).\newline
- Trains and evaluates the model $N$ times, resulting in $N$ evaluation scores.\newline
- It selects a different fold for evaluation every time and trains on the other $N-1$ folds.\newline
\texttt{from sklearn.model\char`_selection import cross\char`_val\char`_score}\newline
\texttt{scores = cross\char`_val\char`_score(model, train\char`_X\char`_prepared, train\char`_y,}\newline
\texttt{.~~~~~~~~~~~~~~~~~~~~~~~~scoring=\textquotesingle neg\char`_mean\char`_squared\char`_error\textquotesingle, cv=10)}\newline
\texttt{rmse\char`_scores = np.sqrt(-scores)}
% \item
% Fine tune model hyperparameters using grid search:\newline
% \texttt{from sklearn.model\char`_selection import GridSearchCV}\newline
% \texttt{dictionary\char`_1 = \{\textquotesingle n\char`_estimators\textquotesingle:[3,10,30], \textquotesingle max\char`_features\textquotesingle:[2,4,6,8]\}}\newline
% \texttt{dictionary\char`_2 = \{\textquotesingle bootstrap\textquotesingle:[False], \textquotesingle n\char`_estimators\textquotesingle:[3,10,30]\}}\newline
% \texttt{param\char`_grid = [dictionary\char`_1, dictionary\char`_2]}\newline
% \texttt{forest\char`_reg = RandomForestRegressor()}\newline
% \texttt{grid\char`_search = GridSearchCV(forest\char`_reg, param\char`_grid, cv=5,}\newline
% \texttt{.~~~~~~~~~~~~~~~~~~~~~~~~~~scoring=\textquotesingle neg\char`_mean\char`_squared\char`_error\textquotesingle,}\newline
% \texttt{.~~~~~~~~~~~~~~~~~~~~~~~~~~return\char`_training\char`_score=True)}\newline
% \texttt{grid\char`_search.fit(train\char`_X\char`_prepared, train\char`_y)}\newline
% \newline
% \texttt{grid\char`_search.best\char`_params\char`_}\newline
% \texttt{grid\char`_search.best\char`_estimator\char`_}\newline
% \texttt{grid\char`_search.cv\char`_results\char`_[\textquotesingle mean\char`_test\char`_score\textquotesingle]}\newline
% \texttt{grid\char`_search.cv\char`_results\char`_[\textquotesingle params\textquotesingle]}
\item
Study the relative importance of each attribute:\newline
\texttt{feat\char`_importances = model.feature\char`_importances}\newline
\texttt{feats = num\char`_attributes + list(cat\char`_encoder.categories\char`_[0]) + ...}\newline
\texttt{sorted(zip(feat\char`_importances, feats), reverse=True)}
\end{itemize}

Evaluate model on the test set.\newline
\texttt{test\char`_y = test[\textquotesingle median\char`_house\char`_value\textquotesingle].copy()}\newline
\texttt{test\char`_X = test.drop(\textquotesingle median\char`_house\char`_value\textquotesingle, axis=1)}\newline
\texttt{test\char`_X\char`_prepared = full\char`_pipeline.\textbf{transform}(test\char`_X)}\newline
\texttt{test\char`_pred\char`_y = final\char`_model.predict(test\char`_X\char`_prepared)}\newline
\texttt{test\char`_mse = mean\char`_square\char`_error(test\char`_y, test\char`_pred\char`_y)}\newline
\texttt{test\char`_rmse = np.sqrt(test\char`_mse)}\newline

Saving models.\newline
\texttt{import joblib}\newline
\texttt{joblib.dump(my\char`_model, \textquotesingle my\char`_model.pkl\textquotesingle)}\newline
\texttt{my\char`_model\char`_loaded = joblib.load(\textquotesingle my\char`_model.pkl\textquotesingle)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Classification}

Training a binary classification model.
Note that \texttt{train\char`_y} should be \texttt{True} or \texttt{False}.\newline
\texttt{model.fit(train\char`_X, train\char`_y)}\newline
\texttt{train\char`_pred\char`_y = model.predict(train\char`_X)}\newline
\texttt{train\char`_prob\char`_y = model.predict\char`_proba(train\char`_X)[:,1]}\newline

Perform K-fold cross-validation:
\vspace{-3.0mm}
\begin{itemize}
\item
Ensure the K-fold cross-validation contains the same percentage of positive classes.\newline
\texttt{from sklearn.model\char`_selection import StratifiedKFold}\newline
\texttt{CV = StratifiedKFold(n\char`_splits=5)}\newline

\item
The following returns the class prediction rather than an evaluation score...\newline
\texttt{from sklearn.model\char`_selection import cross\char`_val\char`_predict}\newline
\texttt{cv\char`_pred\char`_y = cross\char`_val\char`_predict(model, train\char`_X, train\char`_y, cv=CV)}\newline

\item
Or you could get the predicted probability of a positive class...\newline
\texttt{cv\char`_prob\char`_y = cross\char`_val\char`_predict(model, train\char`_X, train\char`_y, cv=CV,\newline
.~~~~~~~~~~~~~~~~~~~~~~~~~~~~~method=\textquotesingle predict\char`_proba\textquotesingle)[:,1]}\newline
\texttt{cv\char`_pred\char`_y = (cv\char`_prob\char`_y > 0.5) \# you could alter this threshold}\newline

\item
Note that some models have a decision function score rather than a probability.\newline
\texttt{cv\char`_score\char`_y = cross\char`_val\char`_predict(model, train\char`_X, train\char`_y, cv=CV,\newline
.~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~method=\textquotesingle decision\char`_function\textquotesingle)}\newline
\texttt{cv\char`_pred\char`_y = (cv\char`_score\char`_y > 0.0) \# you could alter this threshold}

\end{itemize}

Performance Measures:
\vspace{-3.0mm}
\begin{itemize}

\item
Confusion Matrix.\newline
\texttt{from sklearn.metrics import confusion\char`_matrix}\newline
\texttt{confusion\char`_matrix(train\char`_y, cv\char`_pred\char`_y)}

\texttt{~~~~~~~~~~~~~~~~~Predicted \textbf{N}egative~~Predicted \textbf{P}ositive}\newline
\texttt{Actual Negative~|~TN~~~~~~~~~~~~~~~~~FP~~~~~~~~~~~~~~~~}\newline
\texttt{Actual Positive~|~FN~~~~~~~~~~~~~~~~~TP~~~~~~~~~~~~~~~~}\newline

\item
$\textrm{Accuracy} = (\textrm{TP}+\textrm{TN}) / (\textrm{TP}+\textrm{TN}+\textrm{FP}+\textrm{FN})$\newline
Not good for skewed datasets (when some classes are much more frequent than others).

\texttt{from sklearn.metrics import accuracy\char`_score}\newline
\texttt{accuracy\char`_score(train\char`_y, cv\char`_pred\char`_y)}\newline

\item
$\textrm{Precision} = \textrm{TP} / (\textrm{TP}+\textrm{FP})$\newline
The fraction of positive class predictions that are indeed positive in actuality.

\texttt{from sklearn.metrics import precision\char`_score}\newline
\texttt{precision\char`_score(train\char`_y, cv\char`_pred\char`_y)}\newline

\item
$\textrm{Recall} = \textrm{TP} / (\textrm{TP}+\textrm{FN})$. (Also called sensitivity)\newline
The fraction of actual positive classes that are detected.

\texttt{from sklearn.metrics import recall\char`_score}\newline
\texttt{recall\char`_score(train\char`_y, cv\char`_pred\char`_y)}\newline

\item
There is a trade off between precision and recall.
Depending on what you are doing, you might favour one over the other.
E.g. you'd prioritise a high recall when detecting cancer.\newline

\item
F1 is the harmonic mean of precision and recall.
$\textrm{F1} = \frac{2}{\frac{1}{\textrm{precision}} + \frac{1}{\textrm{recall}}}$\newline
You can only get a high F1 score if both precision and recall are high.\newline
\texttt{from sklearn.metrics import f1\char`_score}\newline
\texttt{f1\char`_score(train\char`_y, cv\char`_pred\char`_y)}\newline

\item
Plot precision against recall, or both as a function of the threshold.\newline
\texttt{from sklearn.metrics import precision\char`_recall\char`_curve}\newline
\texttt{precs, recalls, thresh = precision\char`_recall\char`_curve(train\char`_y, cv\char`_prob\char`_y)}\newline
\texttt{ax1.plot(recalls, precs)}\newline
\texttt{...}\newline
\texttt{ax2.plot(thresh, precs[:-1], \textquotesingle b--\textquotesingle, label=\textquotesingle precision\textquotesingle)}\newline
\texttt{ax2.plot(thresh, recalls[:-1], \textquotesingle g--\textquotesingle, label=\textquotesingle recall\textquotesingle)}\newline

\item
Plot a ROC (Receiver Operating Characteristics) curve.\newline
This is the true-positive-rate (recall)
vs the false-positive-rate (fall-out)
$= \textrm{FP} / (\textrm{FP}+\textrm{TN})$.\newline
Fall-out is the probability of a false alarm (not a great measure for skewed datasets).\newline
\texttt{from sklearn.metrics import roc\char`_curve}\newline
\texttt{fpr, tpr, thresh = roc\char`_curve(train\char`_y, cv\char`_prob\char`_y)}\newline
\texttt{ax.plot(fpr, tpr)}\newline
\texttt{ax.set\char`_xlabel(\textquotesingle False Positive Rate (Fall-out)\textquotesingle)}\newline
\texttt{ax.set\char`_ylabel(\textquotesingle True Positive Rate (Recall)\textquotesingle)}\newline
\texttt{...}\newline
The area under this curve can be used as a performance measure.\newline
A perfect classifier would have AUC=1 and a random classifier would have AUC=0.5.
\texttt{from sklearn.metrics import roc\char`_auc\char`_score}\newline
\texttt{roc\char`_auc\char`_score(train\char`_y, cv\char`_prob\char`_y)}

\item
It might be appropriate to consider
the actual number of positives,
the predicted number of positives,
and the predicted total probability.\newline
\texttt{train\char`_y.sum()}\newline
\texttt{cv\char`_pred\char`_y.sum()}\newline
\texttt{cv\char`_prob\char`_y.sum()}\newline

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{SHAP Plots}

Create SHAP plots to explain the output of ML models.

\texttt{import shap}\newline
\newline
\texttt{feats = num\char`_attribs}\newline
\texttt{for i in range (0, len(cat\char`_attribs)):}\newline
\texttt{.~~~feats = feats + list(cat\char`_encoder.categories\char`_[i])}\newline
\newline
\texttt{explainer = shap.TreeExplainer(model)}\newline
\texttt{shap\char`_values = explainer.shap\char`_values(train\char`_X\char`_prepared)}\newline
\newline
\texttt{shap.summary\char`_plot(shap\char`_values,}\newline
\texttt{.~~~~~~~~~~~~~~~~~train\char`_X\char`_prepared,}\newline
\texttt{.~~~~~~~~~~~~~~~~~feature\char`_names=feats,}\newline
\texttt{.~~~~~~~~~~~~~~~~~plot\char`_type=\textquotesingle dot\textquotesingle,}\newline
\texttt{.~~~~~~~~~~~~~~~~~show=False,}\newline
\texttt{.~~~~~~~~~~~~~~~~~max\char`_display=len(feats))}\newline
\texttt{fig = plt.gcf()}\newline
\texttt{fig.tight\char`_layout()}\newline
\texttt{plt.savefig(output\char`_path)}\newline

%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Repository Structure}

The following demos a typical ML repo structure
(only use notebooks for exploratory work):

\texttt{datasets/2020\char`_10\char`_14/COPY\char`_create\char`_dataset.py}\newline
\texttt{datasets/2020\char`_10\char`_14/data\char`_raw.csv}\newline
\texttt{datasets/2020\char`_10\char`_14/data\char`_clean.csv}\newline
\texttt{datasets/2020\char`_10\char`_14/models/}\newline
\texttt{datasets/2020\char`_10\char`_14/models/XGBoost\char`_nEst100/COPY\char`_model\char`_training\char`_and\char`_evaluation.py}\newline
\texttt{datasets/2020\char`_10\char`_14/models/XGBoost\char`_nEst100/model.pkl}\newline
\texttt{datasets/2020\char`_10\char`_14/models/XGBoost\char`_nEst100/model\char`_evalulation.pdf}\newline
\texttt{datasets/2020\char`_10\char`_14/models/XGBoost\char`_nEst100/model\char`_evalulation.txt}\newline
\texttt{sql/redshift\char`_query.sql}\newline
\texttt{create\char`_dataset.py}\newline
\texttt{model\char`_training\char`_and\char`_evaluation.py}\newline

%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Decision Trees}

Decision trees are versatile ML algos that can perform both classification and regression tasks.
They are capable of fitting complex datasets,
but they're also intuitive, with decisions that are easy to interpret
(due to this, they are often called \textit{white box models}).

You can picture a decision tree as a set of nodes
that branch off due to conditions on the feature space.
Root node = depth 0.
Leaf node = does not have any child nodes.\newline

\underline{The Classification And Regression Tree (CART) algorithm:}
\vspace{-3.0mm}
\begin{enumerate}
\item
Split the training set into two subsets using a single feature $k$ and a threshold $t_k$.\newline
Note that this produces binary trees (all nodes can only have two children).
\item
Search for the pair $(k, t_k)$ that produces the purest subsets (weighted by size)\newline
by minimizing:
\begin{equation}
J(k, t_k) = \frac{m_{\textrm{left}}}{m}G_{\textrm{left}} + \frac{m_{\textrm{right}}}{m}G_{\textrm{right}}
\end{equation}
where $m$ is the number of instances
and $G = 1 - \sum p_j^2$ (the Gini impurity)\newline
where $p_j$ is the ratio of class $j$ instances.
NB: a node is pure if $G=0$.
\item
Recursively apply the same logic.
\item
Stop if there isn't a split that reduces impurity
or when hyperparameter contraints are met.
E.g. \texttt{max\char`_depth}, \texttt{max\char`_leaf\char`_nodes}, \texttt{min\char`_samples\char`_leaf}.
\end{enumerate}

The CART algorithm is a greedy algorithm;
it searches for an optimum split at the top level, then repeats.
This is necessary, however, due to the exponential increase in nodes.

Note 1: you can also use entropy to measure the impurity of a node:
$H = - \sum p_j \, \textrm{log}_2 \, p_j$

Note 2: for regression tasks, the CART algorithm tries to minimize MSE rather than impurity.

\newpage
\underline{Classification:}\newline
\texttt{from sklearn.tree import DecisionTreeClassifier}\newline
\texttt{tree\char`_clf = DecisionTreeClassifier(max\char`_depth=2)}\newline
\texttt{tree\char`_clf.fit(X, y)}

The predicted class is that which is most common
in the training instances associated with the corresponding leaf node.\newline
\texttt{tree\char`_clf.predict(X)}

A decision tree can also estimate the probability
that an instance belongs to a particular class $k$.\newline
% 
It finds the leaf node for the instance,
and returns the ratio of class $k$ (training set) instances.\newline
\texttt{tree\char`_clf.predict\char`_proba(X)}\newline

\underline{Regression:}\newline
\texttt{from sklearn.tree import DecisionTreeRegressor}\newline
\texttt{tree\char`_reg = DecisionTreeRegressor(max\char`_depth=2)}\newline
\texttt{tree\char`_reg.fit(X, y)}

The prediction is the average target value of the
training instances associated with the corresponding leaf node.\newline
\texttt{tree\char`_reg.predict(X)}\newline

\underline{Limitations:}
\vspace{-3.0mm}
\begin{itemize}
\item
Decision trees love orthogonal decision boundaries.
Consequently, they are sensitive to training set rotation.
\textit{NB: PCA often results in a better orientation of the training data.}
\item
Decision trees are very sensitive to small variations in the training data.\newline
\textit{NB: Random Forests can limit this instability by averaging predictions over many trees.}
\item
Decision trees make very few assumptions about the training data.
If left unconstrained, the tree structure will adapt itself to the training data,
fitting it very closely - most likely overfitting it.
\textit{Regularization techniques will restrict the algos freedom during training.}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Bias/Variance Trade-off}

A model's generalization error can be expressed as the sum of three very different errors:
\vspace{-3.0mm}
\begin{itemize}
\item
\textbf{Bias:}
this is due to wrong modeling assumptions, such as assuming that the data is linear when it is actually quadratic.
A high-bias model is likely to underfit the training data.
\item
\textbf{Variance:}
this is due to the model's excessive sensitivity to small variations in the training data.
A model with many degrees of freedom is likely to have a high-variance and thus overfit the training data.
\item
\textbf{Irreducible error:}
this is due to the noisiness of the data itself.
\end{itemize}

Increasing a model's complexity will typically increase its variance and reduce its bias.\newline
Conversely, reducing a model's complexity will increase its bias and reduces its variance.\newline
It's a trade-off.

%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ensemble Learning}

If you aggregate the predictions of a group of predictors (such as classifiers or regressors),
you will often get better predictions than with the best individual predictor.
% 
A group of predictors is called an ensemble.
Common ensemble methods are bagging and boosting.\newline

\underline{Bagging/Pasting}\newline
Use the same training algorithm for every predictor,
and train them on different random subsets of the training set.
For classification, use the statistical mode.
For regression, take the average.
\vspace{-3.0mm}
\begin{itemize}
\item
\textbf{Bagging:} when the sampling is performed with replacement (generally preferred).
% The name is short for bootstrap aggregating.
\item
\textbf{Pasting:} when the sampling is performed without replacement.
\end{itemize}
%
\vspace{-3.0mm}
Generally, the net result is that the ensemble has a similar bias but a lower variance
than a single predictor trained on the original training set.

A \textbf{Random Forest} is an ensemble of decision trees,
generally trained via the bagging method.
For greater tree diversity,
extra randomness is introduced when growing the trees
(this increases the bias, but reduces the variance).
\textit{Extremely Randomized Trees} have an even lower variance.

\texttt{from sklearn.ensemble import RandomForestClassifier}\newline
\texttt{model = RandomForestClassifier(n\char`_estimators=500)}

\texttt{from sklearn.ensemble import RandomForestRegressor}\newline
\texttt{model = RandomForestRegressor(n\char`_estimators=500)}\newline

\underline{Boosting}\newline
The general idea of boosting methods is to train predictors sequentially,
each trying to correct its predecessor.
Popular methods include \textit{AdaBoost} and \textit{Gradient Boosting}.

Gradient Boosting works by fitting a new predictor
to the residual errors made by the previous predictor.
% 
It has a regularization technique called \textit{shrinkage},
where a \texttt{learning\char`_rate} hyperparameter scales the contribution of each predictor.
When set to a low value,
more predictors are required in the ensemble to fit the training set,
but the predictions will usually generalize better.
% 
Note that to trade a higher bias for a lower variance,
one can train each predictor on a random subset of the training instances.

XGBoost (which stands for extreme gradient boosting)
is a python library which has an optimized implementation of Gradient Boosting.
Its default is to use tree based models.
Check it out, it has a very similar API to Scikit-Learn.

\texttt{import xgboost}\newline
\newline
\texttt{model = xgboost.XGBRegressor(n\char`_estimators=500)}\newline
\texttt{\# model = xgboost.XGBClassifier(n\char`_estimators=500)}\newline
\newline
\texttt{model.fit(X, y)}\newline
\texttt{y\char`_pred = model.predict(X)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{TODO}



Chapter 4::: training models
Useful for training NEURAL NETWORKS...

Linear Regression:
- 'closed form'
-
Using an iterative optimization approach called Gradient Descent (GD)
that gradually tweaks the model parameters to minimize the cost function over the training set.
(Batch GD, Mini-batch GD, and Stochastic GD)

Linear Regression.
predicted value: y_hat = theta_0 + theta_1.x_1 + ... theta_n.x_n

p113 for equations and stuff

from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X, y)
lin_reg.intercept_
lin_reg.coef_

What if there are a large number of features
or too many training instances to fit in memory?

Gradient Descent:
Gradient descent is a generic optimization algorithm capable of finding the optimal solution to a wide range of problems. 
The general idea is to tweak parameters iteratively in order to minimize a cost function.
Gradient descent measures the local gradient of the error function with regard to the parameter vector THETA,
and it goes in the direction of the descending gradient.

Randomly initialized... wait until it converges!

There is a learning rate hyper parameter, which controls the size of the steps

We want global minimum, not local minimum.

you should feature scale, so the hyperparams are not operating at totaly different scales...

Batch Gradient Descent. p121.
It uses the whole batch of training data at every step.
page 122 has the main equation...I reckon
theta(next step) = theta - eta GRAD MSE(theta)

Stochastic Gradient Descent:
picks a random instance in the training set at every step,
and computes the gradients based only on that single instance.
little data to manipulate = faster (good for huge data sets)
but on the other hand,
due to it's stochastic (ie random) nature,
the algorithm is much less regular.
It will also never truly settle down (can help jump out of local minima)
>>> gradually reduce the learning rate p125 = learning schedule

the concept of an epoch...

Mini-batch Gradient descent:
instead of computing gradients based on the full training set (batch)
or just one random instance (stocahstic)
mini-batch GD computes the gradients on small random sets of instances called mini-batches.
^ less eratic than stochastic GD

A model generalization (total) error has three components:
1. Bias: wrong assumptions lead to underfitting
2. Variance: model's excessive sensitivity to small variations in training data. likely due to overfitting.
3. Irreducible error: nosiness of the data itself


A good way to reduce overfitting is to regularize the model (ie constrain it somehow)
^reducing the degrees of freedom

For a linear model, regularization is typically achived by constraining the weights of the model.
Ridge Regression
Lasso Regression
Elastic Net

Can also do early stopping!
^ Stop training as soon as the validation error reaches the minimum

LOGISTIC REGRESSION p142
Some regression algorithms can be used for classification.
Logisitic regression is commonly used to estimate the probability that
an instance belongs to a particular class.
E.g. if the estimated probability is greater than 50%,
then the model predicts that the instance belongs to that class
^Binary classifier.

p = sigma(x.theta)

sigmoid function:
sigma(t) = 1 / (1 + exp(-t))
Worth noting the shape of this function
t is called the LOGIT

Training and cost function.
p144: cost function & log loss logisitc regression cost function.

Decision boundaries...
Point where classification will change

from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression()
log_reg.fit(X, y)
log_reg.predict(X_new) # classification
log_reg.predict_proba(X_new) # 

Logisitc regression models can be regularised using l1 or l2 penalities...
scikit learn actually adds an l2 penalty by default.

SOFTMAX REGRESSION
The logistic regression model can be generalized to support multiple classes directly
without having to train and combine multiple binary classifiers (as discussed previous).
This is called softmax regression OR multinomial logistic regression.
p148-p151 very useful...

softmax_reg = LogisticRegression(multi_class="multinomial", solver="lbfgs", C=10)




















---
p213 dimensionality reduction
---

Many ML problems involve thousands or even millions of features for each training instance.
Not only do all these features make training extremely slow,
but they make it much marder to find a good solution.
THIS IS THE CURSE OF DIMENSIONALITY.

e.g. merging pixels, ignoring outer pixels...

Reducing dimensionality does cause some information loss...
It will speed up training, but probably make your system perform slightly worse.

Dimensionality reduction is also extremely useful for data visulization (DataViz)
^ can only really interpret plots in 2D ::: can visually detect patterns, such as clusters.


The curse of dimensionality:
High-dimensional datasets are at risk of being very sparse:
most training instances are likely to be far away from each other.
This also means that a new instance will likely be far away from any training instance,
mkaing predictions much less reliable than in lower dimensions,
since they will be based on much larger extrapolations.
In short, the more dimensions a training set has,
the greater the risk of overfitting it.

In theory,
one solution would be to increase the size of the training set to reach sufficient density of training instances.
Unfortunately,
the number of training instances required to reach a given density grows exponentially with the number of dimensions.

Main approaches for dimensionality reduction:
Projection AND Manifold Learning

Projection:
In most real-world problems,
training instances are NOT spread out uniformly across all dimensions.
Many features are almost constant,
while others are highly correlated.
As a result, all training instances lie within (or close to)
a much lower-dimensional SUBSPACE of the high-dimensional space.

e.g. 3d but on a 2d plane
But this wouldn't work for 2d data 'rolled up' in 3d

MANIFOLD LEARNING
A d-dimensional manifold is part of an n-dimensional space (where d < n)
that LOCALLY resembles a d-dimensional hyperplane
(but on a larger scale it can bend and twist in the n-dimensional space.)
Manifold hypothesis:
Most real-world hign-dimensionnal datasets lie close to a much lower-dimesnional manifold.

PCA: p219.
Principal Component Analysis is by far the most popular dimensionality reduction algorithm.
It identifies the hyperplane that lies closest to the data,
and then projects the data onto it.

PCA identifies the axis that accounts for the largest amount of variance in the training set.
It then finds a second axis, orthogonal to the first one,
that accounts for the largest amout of remaining variance. ETC.

The i^th axis is called the i^th principal component of the data.

Once you have identified all n principal components,
you can reduce the dimensionality of the dataset down to d dimensions,
by projecting it onto the hyperplane defined by the first d principal components. (d < n)

from sklearn.decomposition import PCA
pca = PCA(n_components = 2)
X2D = pca.fit_transform(X)
pca.explained_variance_ratio_ (the proportion of the dataset's variance that lies along each principle component)

CHOOSING THE RIGHT NUMBER OF DIMENSIONS
for example, try and capture 95% of the variance
pca = PCA(n_components=0.95)
X_reduced = pca.fit_transform(X_train)
OBV for dataVis it's gotta be 2 or 3

could also plot explained variance VS number of dimensions,
there will usually be an elbow in the curve.

PCA for compression (and decompression):
pca = PCA(n_components = 154)
X_reduced = pca.fit_transform(X_train)
X_recovered = pca.inverse_transform(X_reduced)

Can make it non-linear using kernel techniques:
from sklearn.decomposition import KernelPCA
rbf_pca = KernelPCA(n_components=2, kernel='rbf', gamma=0.04)
X_reduced = rbf_pca.fit_transform(X)

LLE: Locally Linear Embedding
is another powerful nonlinear dimensionality reduction technique.
It is a manifold technique that does not rely on projections.
Particularly good at unrolling twisted manifolds, especially when there is not too much noise.

from sklearn.manifold import LocallyLinearEmbedding
lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)
X_reduced = lle.fit_transform(X)

ALSO t-Distirbuted Stochastic Neighbor Embedding
*** I HAVE USED THIS DURING CLUSTERING WORK!





Unsupervised Learning Techniques: Chapter9
Clustering: the goal is to group similar instances together into clusters.
Anomaly Detection: The objective is to learn what 'normal' data looks like,
then use that to detect abnormal instances look like
Density estimation: this is the task of estimating the probability density function of the random process generating the dataset..

CLUSTERING

Applications:
Customer segmentation
Dimensionality reduction (soft clustering)
Outlier detection (low affinity instances)
Semi supervised: if you only have a few labels, perform clustering and then propagate the labels to all the instances in the same cluster
Search engines...

There is no universal definition of what a cluster is...

*** ALSO LOOK AT YOUR CLUSTERING WORK HERE ***
KMEANS:
from sklearn.cluster import KMeans
k = 5
kmeans = KMeans(n_clusters=k)
y_pred = kmeans.fit_predict(X)

kmeans.cluster_centres_

Note that you must specifiy the number of clusters, k, that the algorithm must find

K-Means algorithm does not behave very well when the blobs have very different diameters because all it cares about
when assigning an instance to a cluster is the distance to the centroid.
Instead of assigning each instance to a single cluster, which is called hard-clustering,
it can be useful to give each instance a score per cluster, which is called soft-clustering.
^Can be a distance or an affinity. NB: this can be a dimensionality reduction technique.
kmeans.transform(X)

Kmeans is guarenteed to converge, but it might find a local optimum.
Dependent on the random initialization step
^^^ Run the algorithm multiple times with different random initializations and keep the best solution
n_init hyperparameter (which is ten by default)

performance measure = INERTIA
^the mean squared distance between each instance and its closest centroid
Keep model with the lowest inertia
kmeans.inertia_
NB kmeans.score(X) = NEGATIVE INERTIA

there is actually an initilization algo under the bonnet
to ensure much less likely to converge on a suboptimal solution...
AND an accelerated distance calculation going on!

FINDING THE OPTIMAL NUMBER OF CLUSYERS.
Can't just pick k which with lowest inertia
(It keeps getting lower as you increase k)
^ could look at the 'elbow', but this is rather coarse

A more precise approach is to use the silhouette coefficient
and silhouette score (mean silhouette coefficient over all instances)

A silhouette coefficient can vary between -1 and +1.
A coefficient of +1 means that the instances is well inside its own cluster, and far from others.
A coefficient of ~0 means that it is close to a cluster boundary
A coefficient of -1 means that the instance may have been assigned to the wrong cluster!

from sklearn.metrics import silhouette_score
silhouette_score(X, kmeans.labels_)
plot silhouette score as a function of k...

Silhouette diagram:
the shape's height indicates the number of instances
the cluster contains, and its width presents the sorted silhouette coefficients o fht einstances in the cluster.
The vertical dashed lines represent the silhouette score for each number of clusters.
When most of the instances in a cluster have a lower coefficient than this score,
then the cluster is bad.
Also, consider the size of the clusters...

Kmeans not great when clusters have varying sizes,
different densities,
or nonspherical shapes...
* IT IS IMPORTANT TO SCALE INPUT FEATURES BEFORE YOU RUN KMEANS

:::semi-supervised learning:::
use to create labels for all data, when you only have a few!

DBSCAN: defines clusters as continuous regions of high density, which is good for clusters of arbitary shapes.
AGGLOMERATIVE: hierachy of clusters is built from the bottom up
^ HARDER TO OPTIMISE
&& can't get soft-clustering results!

Instead of distance based clustering
there are hierachical clustering methods


Gaussian Mixtures:
GMM is a probabilistic model that assumes that the instances were generated from a mixture of several gaussian distributions
whose parameters are unknown.
You must know in advance the number k of Gaussian distributions

p268
Given a statistical model with some parameters, theta, the word probabiltiy is used to describe low plausible a future outcome, x, is.
The word likelihood is used to describe how plausible a particular set of paramter values, theta, are after the outcome x is known.
a PDF is a function of x, with theta fixed
a likelihood function is a functions of theta, with x fixed.
















































































